# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dpOuQctnaBWJtpfD_TYusFdCn67hogeB
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

df = pd.read_csv('Finance.csv')

# Basic statistics
df.describe(include='all')

df.head()

# Check class distribution of target variable
df['Loan_Status'].value_counts(normalize=True)

# Visualize missing values
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

plt.figure(figsize=(10, 6))
sns.heatmap(df.isnull(), cbar=False, yticklabels=False, cmap='viridis')
plt.title('Missing Values')
plt.tight_layout()

# Distribution of numerical features
plt.figure(figsize=(15, 10))
for i, col in enumerate(['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', ]):
    plt.subplot(2, 2, i+1)
    sns.histplot(df[col], kde=True)
    plt.title(f'Distribution of {col}')
plt.tight_layout()

# Relationship between numerical features and target
plt.figure(figsize=(15, 10))
for i, col in enumerate(['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term']):
    plt.subplot(2, 2, i+1)
    sns.boxplot(x='Loan_Status', y=col, data=df)
    plt.title(f'{col} vs Loan Status')
plt.tight_layout()

# Categorical features analysis
plt.figure(figsize=(15, 15))
categorical_cols = ['Gender', 'Married', 'Dependents', 'Education',
                   'Self_Employed', 'Credit_History', 'Property_Area']
for i, col in enumerate(categorical_cols):
    plt.subplot(3, 3, i+1)
    cross_tab = pd.crosstab(df[col], df['Loan_Status'], normalize='index')
    cross_tab.plot(kind='bar', stacked=True)
    plt.title(f'{col} vs Loan Status')
plt.tight_layout()

# Correlation matrix
plt.figure(figsize=(10, 8))
numeric_df = df.select_dtypes(include=[np.number])
sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.tight_layout()

# Impute categorical variables with mode
for col in ['Gender', 'Married', 'Dependents', 'Self_Employed', 'Credit_History', 'Property_Area']:
    if df[col].isnull().sum() > 0:
        df[col] = df[col].fillna(df[col].mode()[0])

# Impute numerical variables with median (more robust to outliers than mean)
for col in ['LoanAmount', 'Loan_Amount_Term']:
    if df[col].isnull().sum() > 0:
        df[col] = df[col].fillna(df[col].median())

# Create Total Income feature
df['TotalIncome'] = df['ApplicantIncome'] + df['CoapplicantIncome']

# Create income to loan ratio
df['Income_to_Loan_Ratio'] = df['TotalIncome'] / df['LoanAmount']

# Log transformation for skewed numerical features
for col in ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'TotalIncome']:
    df[f'Log_{col}'] = np.log1p(df[col])  # log1p handles zero values

# Convert 'Dependents' to numerical
df['Dependents'] = df['Dependents'].replace('3+', '3')
df['Dependents'] = pd.to_numeric(df['Dependents'])

# Convert Loan_Status to binary (target encoding)
df['Loan_Status'] = df['Loan_Status'].map({'Y': 1, 'N': 0})

# One-hot encoding for categorical variables
categorical_cols = ['Gender', 'Married', 'Education', 'Self_Employed', 'Property_Area']
df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

from sklearn.preprocessing import StandardScaler

# Scale numerical features
numerical_cols = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount',
                 'Loan_Amount_Term', 'TotalIncome', 'Income_to_Loan_Ratio',
                 'Log_ApplicantIncome', 'Log_CoapplicantIncome', 'Log_LoanAmount', 'Log_TotalIncome']
scaler = StandardScaler()
df[numerical_cols] = scaler.fit_transform(df[numerical_cols])

from sklearn.feature_selection import SelectKBest, f_classif

# Separate features and target
X = df.drop(['Loan_ID', 'Loan_Status'], axis=1)
y = df['Loan_Status']

# Select top k features
selector = SelectKBest(score_func=f_classif, k=10)
X_new = selector.fit_transform(X, y)

# Get selected feature names
selected_features = X.columns[selector.get_support()].tolist()
print("Selected features:", selected_features)

from sklearn.utils import resample
from imblearn.over_sampling import SMOTE

# Check for imbalance
print(df['Loan_Status'].value_counts())

# Option 1: SMOTE
smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X, y)

# Option 2: Upsampling minority class
if df['Loan_Status'].value_counts()[0] != df['Loan_Status'].value_counts()[1]:
    # Separate majority and minority classes
    df_majority = df[df['Loan_Status']==df['Loan_Status'].value_counts().idxmax()]
    df_minority = df[df['Loan_Status']==df['Loan_Status'].value_counts().idxmin()]

    # Upsample minority class
    df_minority_upsampled = resample(df_minority,
                                    replace=True,
                                    n_samples=len(df_majority),
                                    random_state=42)

# Combine majority class with upsampled minority class
df_balanced = pd.concat([df_majority, df_minority_upsampled])

from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from xgboost import XGBClassifier

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define models to try
models = {
    'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),
    'RandomForest': RandomForestClassifier(random_state=42),
    'GradientBoosting': GradientBoostingClassifier(random_state=42),
    'XGBoost': XGBClassifier(random_state=42)
}

# Compare models with cross-validation
for name, model in models.items():
    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
    print(f"{name} CV Accuracy: {scores.mean():.4f} (+/- {scores.std():.4f})")

# Hyperparameter tuning for best model (example for RandomForest)
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 5, 10, 15],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(RandomForestClassifier(random_state=42),
                          param_grid=param_grid,
                          cv=5,
                          scoring='accuracy',
                          n_jobs=-1)

grid_search.fit(X_train, y_train)
print(f"Best parameters: {grid_search.best_params_}")
print(f"Best cross-validation score: {grid_search.best_score_:.4f}")


# Evaluate best model
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

print(f"Test Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# Feature importance
if hasattr(best_model, 'feature_importances_'):
    importances = best_model.feature_importances_
    indices = np.argsort(importances)[::-1]

    plt.figure(figsize=(10, 6))
    plt.title('Feature Importances')
    plt.bar(range(X_train.shape[1]), importances[indices])
    plt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)
    plt.tight_layout()

